# ReadingList
Paper and talk grass list.
### 06142017

[Notes on Noise Contrastive Estimation and Negative Sampling](https://arxiv.org/pdf/1410.8251.pdf)

[A Scalable Hierarchical Distributed Language Model](https://pdfs.semanticscholar.org/1005/645c05585c2042e3410daeed638b55e2474d.pdf)

[Learning word embeddings efficiently with
noise-contrastive estimation](https://papers.nips.cc/paper/5165-learning-word-embeddings-efficiently-with-noise-contrastive-estimation.pdf)

[importance sampling](ieeexplore.ieee.org/iel5/72/4479602/04443871.pdf)

### 06132017

[perlexity mit notes](http://web.mit.edu/6.863/www/fall2012/lectures/lecture2&3-notes12.pdf)

[nlp stanford slides](https://web.stanford.edu/class/cs124/lec/languagemodeling.pdf)

[skip thought vectors](https://arxiv.org/pdf/1506.06726.pdf)

[pyTorch Tutorial](http://pytorch.org/tutorials/)
### 06122017

[Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/pdf/1502.03167.pdf)

[Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks](https://arxiv.org/pdf/1506.01497.pdf)

#### Arxiv Lastest

[Unsupervised object learning from dense equivariant
image labelling](https://arxiv.org/pdf/1706.02932.pdf)

[A Joint Model for Question Answering and Question Generation](https://arxiv.org/pdf/1706.01450.pdf)

[ShiftCNN: Generalized Low-Precision Architecture
for Inference of Convolutional Neural Networks](https://arxiv.org/pdf/1706.02393.pdf)

### 06112017

[The Great A.I. Awakening](https://www.nytimes.com/2016/12/14/magazine/the-great-ai-awakening.html?utm_source=wanqu.co&utm_campaign=Wanqu+Daily&utm_medium=social&_r=0)

[Self-Normalizing Neural Networks](https://arxiv.org/pdf/1706.02515.pdf)

[See, Hear, and Read: Deep Aligned Representations](https://arxiv.org/pdf/1706.00932.pdf)

### 06092017

[Deep Learning: A Bayesian Perspective <this>](https://arxiv.org/pdf/1706.00473.pdf)

[Perplexity](https://en.wikipedia.org/wiki/Perplexity)

### 06082017

[distributed mini-batch](https://research.fb.com/wp-content/uploads/2017/06/imagenet1kin1h3.pdf)

[highway network concised](https://arxiv.org/pdf/1505.00387.pdf)

[highway network full paper](https://arxiv.org/pdf/1507.06228.pdf)

[Deep Image: Scaling up Image Recognition](https://arxiv.org/vc/arxiv/papers/1501/1501.02876v1.pdf)


### 06072017

Leaky relu

[Res Net read again](https://arxiv.org/pdf/1512.03385.pdf)

[Know about floating numbers](http://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html)

#### Batch-size selections:
[basic hyper-parameters](http://cs231n.github.io/neural-networks-3/)

[Practical Recommendations for Gradient-Based Training of Deep Architectures](https://arxiv.org/pdf/1206.5533.pdf)

[Stochastic Gradient Descent Tricks](https://www.microsoft.com/en-us/research/wp-content/uploads/2012/01/tricks-2012.pdf)

[Efficient BackProb](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)

[Efficient Mini-batch Training for Stochastic Optimization](http://www.cs.cmu.edu/~muli/file/minibatch_sgd.pdf)

[Optimization Methods for Large-Scale Machine Learning](https://arxiv.org/abs/1606.04838)

[On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima](https://arxiv.org/pdf/1609.04836.pdf)

https://stats.stackexchange.com/questions/140811/how-large-should-the-batch-size-be-for-stochastic-gradient-descent

https://www.quora.com/Intuitively-how-does-mini-batch-size-affect-the-performance-of-stochastic-gradient-descent

https://stats.stackexchange.com/questions/164876/tradeoff-batch-size-vs-number-of-iterations-to-train-a-neural-network




### 06062017

[Very Deep Convolutional Networks for Text Classification](https://arxiv.org/pdf/1606.01781.pdf)

### Long time no read

[optimizing-gradient-descent](http://sebastianruder.com/optimizing-gradient-descent/)

[Exploring the Limits of Language Modeling](https://arxiv.org/pdf/1602.02410.pdf)



## History List
[Topics in ML](https://www.cs.toronto.edu/~duvenaud/courses/csc2541/index.html)

##This Semester
[Visual Dialog](https://arxiv.org/pdf/1611.08669v2.pdf)

[ORDER-EMBEDDINGS OF IMAGES AND LANGUAGE](https://arxiv.org/pdf/1511.06361v6.pdf)

[Learning Aligned Cross-Modal Representations from Weakly Aligned Data](http://www.cs.toronto.edu/~castrejon/content/cvpr2016.pdf)

[Pixel Recursive Super Resolution](https://arxiv.org/pdf/1702.00783v1.pdf) 

[Tutorial on GAN](https://arxiv.org/pdf/1701.00160v3.pdf)

[NIPS 2016 GAN Papers](https://sites.google.com/site/nips2016adversarial/home/accepted-papers)

[Generating Text via Adversarial Training](https://c4209155-a-62cb3a1a-s-sites.googlegroups.com/site/nips2016adversarial/WAT16_paper_20.pdf?attachauth=ANoY7cpDgqvVi1CaTdC4YxVV7h-CfamMefA7wgvpcFFKnycvhSzluG6nOHAjy7Tp1bCIPsruuWBTKaNbZTgnEolWqBGaI7SiFefiS0otYRUM_fu-Fd1lMgLBK6uJHHGPOzTi85LDj4Pj_DpTeGbGTKWDSHbjCMT4XcIiMUONvwicj4wxwf1y1X1greT0T2DmBtmIjh6e1WfFCHKWwBslkh57PqKbD-Z2bnkINNeyJ8Ndj9vEkOPn_FM%3D&attredirects=0)

[Skip-Thought Vectors](https://arxiv.org/pdf/1506.06726.pdf)

[GAN](http://datascienceassn.org/sites/default/files/Generative%20Adversarial%20Nets.pdf)

[Modeling documents with Generative Adversarial Networks](https://c4209155-a-62cb3a1a-s-sites.googlegroups.com/site/nips2016adversarial/WAT16_paper_19.pdf?attachauth=ANoY7cpiEhgAdlzMTtlEVkXEJqYZj9yRyLYZ_pw4OyrFE9OhT_qu7TOZPgmGgu5u3GYb6oz_uqVWEF5zGzzVUVadoWD_qaKRys2vovtk8RPtS_b2JqZP2YVbu6SdAwXi_1bn2XUfO2xIYQ-LP4SpR3-Yjq_n1tN9vIm3uGG5A2LXJcSkzMDIvZO7ojMyWkusYwhXZu3zLNem7JazCSoBCfSKGOlw4jU_6vLY5OhI_fm8vOL1r9PefBI%3D&attredirects=0)

[Tony Wu](https://arxiv.org/pdf/1611.04273.pdf)
###Language & RNN

[Distributed Representations of Sentences and Documents](http://cs.stanford.edu/~quocle/paragraph_vector.pdf)

###Recommendations
[NetEase](http://bmc.uestc.edu.cn/~zhangdongxiang/papers/ICDE16_industry_231.pdf)

[Maksims_1](https://pdfs.semanticscholar.org/c52e/f5426715984b1c6440a582499a549d33e4ce.pdf)

[Effective Latent Models for Binary Feedback in Recommender Systems](https://pdfs.semanticscholar.org/49ee/8a342cb3676f334165aa2dd05fab995c00f7.pdf)

[SESSION-BASED RECOMMENDATIONS WITH RECURRENT NEURAL NETWORKS](https://arxiv.org/pdf/1511.06939.pdf)
## Videos
[Ruslan's Tutorial in Deep Learning](https://simons.berkeley.edu/talks/tutorial-deep-learning)

[Stanford NLP Deep Learning syllabus](http://cs224d.stanford.edu/syllabus.html)

[Stanford NLP Deep Learning video list](https://www.youtube.com/playlist?list=PLlJy-eBtNFt4CSVWYqscHDdP58M3zFHIG)

## For Sanja's Project

[VQA overview](https://arxiv.org/pdf/1607.05910.pdf)

[Multi-labeled CNN](https://arxiv.org/pdf/1406.5726.pdf)

### Follow up of VQA Overview
(https://arxiv.org/pdf/1511.02799.pdf)

(https://cs.stanford.edu/people/karpathy/nips2014.pdf)

(https://arxiv.org/pdf/1505.02074.pdf)

(https://arxiv.org/pdf/1506.07285.pdf)

(https://arxiv.org/pdf/1603.01417.pdf)

***(https://arxiv.org/pdf/1511.06973.pdf)***

***(https://arxiv.org/pdf/1505.05612.pdf)***

***(https://arxiv.org/pdf/1606.01847.pdf)***

[UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS](https://arxiv.org/pdf/1511.06434.pdf)

=======================================================================================

(https://arxiv.org/pdf/1511.05960.pdf)

(https://arxiv.org/pdf/1602.04341.pdf)

(https://arxiv.org/pdf/1512.05193.pdf)

(https://arxiv.org/pdf/1502.03044.pdf)

**[Stacked Attention Networks for Image Question Answering](https://arxiv.org/pdf/1511.02274.pdf)

(https://arxiv.org/pdf/1610.01076.pdf)
